{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbfcc2a",
   "metadata": {},
   "source": [
    "Scraper Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b371b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Login complete.\n",
      "[INFO] Scraping profile (1/1): https://www.linkedin.com/in/vansh-singh-77988b235/\n",
      "[DEBUG] Sleeping for 5.4s...\n",
      "[INFO] Scraped data saved to linkedin_profiles.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import undetected_chromedriver as uc\n",
    "    _HAS_UC = True\n",
    "except Exception:\n",
    "    _HAS_UC = False\n",
    "\n",
    "INPUT_CSV = \"linkedin_profiles.csv\"   # Input CSV file with 'url' column\n",
    "SAVE_RAW_HTML = False\n",
    "HEADLESS = False\n",
    "MIN_DELAY = 3\n",
    "MAX_DELAY = 6\n",
    "LOGIN_TIMEOUT = 30\n",
    "PAGE_TIMEOUT = 30\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:117.0) Gecko/20100101 Firefox/117.0\",\n",
    "]\n",
    "PROXIES = []\n",
    "\n",
    "\n",
    "def get_credentials() -> Dict[str, str]:\n",
    "    os.environ[\"LI_USER\"] = \"santosh3112sing@gmail.com\"\n",
    "    os.environ[\"LI_PASS\"] = \"Scraper_Vansh\"\n",
    "    user = os.getenv(\"LI_USER\")\n",
    "    pwd = os.getenv(\"LI_PASS\")\n",
    "    if not user or not pwd:\n",
    "        raise RuntimeError(\"Set LI_USER and LI_PASS environment variables.\")\n",
    "    return {\"username\": user, \"password\": pwd}\n",
    "\n",
    "\n",
    "def build_driver(headless=True, user_agent=None, proxy=None):\n",
    "    chromedriver_path = \"C:\\\\ChromeDriver\\\\chromedriver.exe\"\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    if user_agent:\n",
    "        options.add_argument(f\"user-agent={user_agent}\")\n",
    "    if proxy:\n",
    "        options.add_argument(f\"--proxy-server={proxy}\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    service = Service(executable_path=chromedriver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def linkedin_login(driver: WebDriver, username: str, password: str) -> None:\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    wait = WebDriverWait(driver, LOGIN_TIMEOUT)\n",
    "    email_in = wait.until(EC.presence_of_element_located((By.ID, \"username\")))\n",
    "    email_in.clear()\n",
    "    email_in.send_keys(username)\n",
    "    pwd_in = driver.find_element(By.ID, \"password\")\n",
    "    pwd_in.clear()\n",
    "    pwd_in.send_keys(password)\n",
    "    pwd_in.send_keys(Keys.RETURN)\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"global-nav-search\")))\n",
    "    except Exception:\n",
    "        time.sleep(3)\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, driver: WebDriver, linkedin_url: str):\n",
    "        self.driver = driver\n",
    "        self.linkedin_url = linkedin_url\n",
    "        self.name = None\n",
    "        self.location = None\n",
    "        self.about = None\n",
    "        self.experiences = []\n",
    "        self.educations = []\n",
    "        self.skills = []\n",
    "        self.publications = []\n",
    "        self.volunteering = []\n",
    "\n",
    "    def _scroll_page(self):\n",
    "        for _ in range(3):\n",
    "            self.driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight/3);\")\n",
    "            time.sleep(1.5)\n",
    "\n",
    "    def extract_basic_info(self):\n",
    "        try:\n",
    "            top_card = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"main\"))\n",
    "            )\n",
    "            self.name = top_card.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "            self.location = top_card.find_element(\n",
    "                By.XPATH, \".//span[contains(@class,'text-body-small') and contains(@class,'t-black--light')]\"\n",
    "            ).text.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def extract_about(self):\n",
    "        try:\n",
    "            about_section = self.driver.find_element(By.ID, \"about\")\n",
    "            about_parent = about_section.find_element(By.XPATH, \"..\")\n",
    "            self.about = about_parent.text.replace(\"About\", \"\").strip()\n",
    "        except Exception:\n",
    "            self.about = None\n",
    "\n",
    "    def extract_section(self, subpath: str) -> List[str]:\n",
    "        try:\n",
    "            url = os.path.join(self.linkedin_url, subpath)\n",
    "            self.driver.get(url)\n",
    "            self._scroll_page()\n",
    "            items = self.driver.find_elements(By.CLASS_NAME, \"pvs-list__paged-list-item\")\n",
    "            return [item.text.strip() for item in items if item.text.strip()]\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    def extract_experience(self):\n",
    "        self.experiences = self.extract_section(\"details/experience\")\n",
    "\n",
    "    def extract_education(self):\n",
    "        self.educations = self.extract_section(\"details/education\")\n",
    "\n",
    "    def extract_skills(self):\n",
    "        self.skills = self.extract_section(\"details/skills\")\n",
    "\n",
    "    def extract_publications(self):\n",
    "        self.publications = self.extract_section(\"details/accomplishments/publications\")\n",
    "\n",
    "    def extract_volunteering(self):\n",
    "        self.volunteering = self.extract_section(\"details/volunteering-experiences\")\n",
    "\n",
    "    def scrape_all(self):\n",
    "        self.driver.get(self.linkedin_url)\n",
    "        self.extract_basic_info()\n",
    "        self.extract_about()\n",
    "        self.extract_experience()\n",
    "        self.extract_education()\n",
    "        self.extract_skills()\n",
    "        self.extract_publications()\n",
    "        self.extract_volunteering()\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"location\": self.location,\n",
    "            \"about\": self.about,\n",
    "            \"experience\": \" | \".join(self.experiences),\n",
    "            \"education\": \" | \".join(self.educations),\n",
    "            \"skills\": \", \".join(self.skills),\n",
    "            \"publications\": \" | \".join(self.publications),\n",
    "            \"volunteering\": \" | \".join(self.volunteering)\n",
    "        }\n",
    "\n",
    "\n",
    "def scrape_profiles_from_csv(csv_path: str):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"url\" not in df.columns and \"URL\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'url' column with LinkedIn profile links.\")\n",
    "    if \"URL\" in df.columns:\n",
    "        df.rename(columns={\"URL\": \"url\"}, inplace=True)\n",
    "\n",
    "    creds = get_credentials()\n",
    "    ua = random.choice(USER_AGENTS)\n",
    "    proxy = random.choice(PROXIES) if PROXIES else None\n",
    "\n",
    "    driver = build_driver(headless=HEADLESS, user_agent=ua, proxy=proxy)\n",
    "    linkedin_login(driver, creds[\"username\"], creds[\"password\"])\n",
    "    print(\"[INFO] Login complete.\")\n",
    "\n",
    "    scraped_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        profile_url = row[\"url\"]\n",
    "        print(f\"[INFO] Scraping profile ({idx+1}/{len(df)}): {profile_url}\")\n",
    "        try:\n",
    "            person = Person(driver, profile_url)\n",
    "            person.scrape_all()\n",
    "            data = person.to_dict()\n",
    "            scraped_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed for {profile_url}: {e}\")\n",
    "            scraped_data.append({key: None for key in [\n",
    "                \"name\", \"location\", \"about\", \"experience\",\n",
    "                \"education\", \"skills\", \"publications\", \"volunteering\"\n",
    "            ]})\n",
    "        delay = random.uniform(MIN_DELAY, MAX_DELAY)\n",
    "        print(f\"[DEBUG] Sleeping for {delay:.1f}s...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    scraped_df = pd.DataFrame(scraped_data)\n",
    "    updated_df = pd.concat([df.reset_index(drop=True), scraped_df], axis=1)\n",
    "    updated_df.to_csv(csv_path, index=False)\n",
    "    print(f\"[INFO] Scraped data saved to {csv_path}\")\n",
    "    return updated_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_profiles_from_csv(INPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473248d0",
   "metadata": {},
   "source": [
    "Converting Raw Scraped Content to Meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c426142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_CSV = \"linkedin_profiles.csv\"\n",
    "OUTPUT_CSV = \"linkedin_profiles_with_report.csv\"\n",
    "TIME_LIMIT = 600 \n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "reports = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = f\"\"\"\n",
    "Generate a detailed professional report for the following LinkedIn profile:\n",
    "\n",
    "Full Name: {row.get('name', '')}\n",
    "Location: {row.get('location', '')}\n",
    "About: {row.get('about', '')}\n",
    "Experience: {row.get('experience', '')}\n",
    "Education: {row.get('education', '')}\n",
    "Skills: {row.get('skills', '')}\n",
    "Publications: {row.get('publications', '')}\n",
    "Volunteering: {row.get('volunteering', '')}\n",
    "\n",
    "Include sections:\n",
    "1. Executive Summary\n",
    "2. Career Timeline\n",
    "3. Research & Technical Highlights\n",
    "4. Key Skills & Strengths\n",
    "5. Education\n",
    "6. Final Impression\n",
    "\n",
    "Write accurately, concisely, and only based on the above information.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"tinyllama\"],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            timeout=TIME_LIMIT\n",
    "        )\n",
    "        report_text = result.stdout.strip()\n",
    "        print(report_text)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"[ERROR] Model execution timed out for row {idx}!\")\n",
    "        report_text = \"\"\n",
    "\n",
    "    reports.append(report_text)\n",
    "\n",
    "df[\"Generated_Report\"] = reports\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31f389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
